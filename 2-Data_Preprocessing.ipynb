{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c78a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import scipy.sparse as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e43d1162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns count: 11\n",
      "First 50 columns: ['id', 'name', 'city', 'rating', 'rating_count', 'cost', 'cuisine', 'lic_no', 'link', 'address', 'menu']\n",
      "'orig_index' in header? False\n"
     ]
    }
   ],
   "source": [
    "# CELL A1 — show header and number of columns (no heavy memory)\n",
    "\n",
    "fn = r\"D:\\python_programs\\Swiggy Recommendation\\cleaned_data.csv\"\n",
    "hdr = pd.read_csv(fn, nrows=0)\n",
    "print(\"Columns count:\", len(hdr.columns))\n",
    "print(\"First 50 columns:\", hdr.columns[:50].tolist())\n",
    "print(\"'orig_index' in header?\", \"orig_index\" in hdr.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0130090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse shape: (108369, 894) nnz: 216738\n",
      "Saved: encoded_features.npz\n",
      "Saved: encoded_feature_names.pkl\n",
      "Saved: encoded_other_columns.csv\n",
      "Saved: encoder.pkl\n",
      "Saved bundle: encoded_bundle.npz\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#   FULL SPARSE ENCODING PIPELINE — NO PARQUET, NO EXTRA INSTALLS\n",
    "# ================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "INPUT = \"cleaned_data.csv\"\n",
    "\n",
    "OUT_SPARSE = \"encoded_features.npz\"\n",
    "OUT_FEATNAMES = \"encoded_feature_names.pkl\"\n",
    "OUT_NONENC = \"encoded_other_columns.csv\"       # CSV instead of parquet\n",
    "ENCODER_PATH = \"encoder.pkl\"\n",
    "OUT_BUNDLE = \"encoded_bundle.npz\"\n",
    "\n",
    "cat_cols = [\"city\", \"cuisine\"]\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD CLEANED DATA\n",
    "# -------------------------------\n",
    "\n",
    "df = pd.read_csv(INPUT)\n",
    "df[\"orig_index\"] = df.index\n",
    "\n",
    "# check cat columns\n",
    "missing = [c for c in cat_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(\"Missing categorical columns: \" + \", \".join(missing))\n",
    "\n",
    "other_cols = [c for c in df.columns if c not in cat_cols]\n",
    "\n",
    "# -------------------------------\n",
    "# ONE-HOT ENCODING (sparse)\n",
    "# -------------------------------\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "encoded_sparse = encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "feat_names = encoder.get_feature_names_out(cat_cols)\n",
    "\n",
    "print(\"Sparse shape:\", encoded_sparse.shape, \"nnz:\", encoded_sparse.nnz)\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE SPARSE MATRIX\n",
    "# -------------------------------\n",
    "\n",
    "sp.save_npz(OUT_SPARSE, encoded_sparse, compressed=True)\n",
    "print(\"Saved:\", OUT_SPARSE)\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE FEATURE NAMES\n",
    "# -------------------------------\n",
    "\n",
    "with open(OUT_FEATNAMES, \"wb\") as f:\n",
    "    pickle.dump(list(feat_names), f)\n",
    "print(\"Saved:\", OUT_FEATNAMES)\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE OTHER (non-encoded) COLUMNS\n",
    "# -------------------------------\n",
    "\n",
    "df[other_cols].to_csv(OUT_NONENC, index=False)\n",
    "print(\"Saved:\", OUT_NONENC)\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE ENCODER\n",
    "# -------------------------------\n",
    "\n",
    "with open(ENCODER_PATH, \"wb\") as f:\n",
    "    pickle.dump(encoder, f)\n",
    "print(\"Saved:\", ENCODER_PATH)\n",
    "\n",
    "# -------------------------------\n",
    "# OPTIONAL: SINGLE BUNDLE NPZ\n",
    "# -------------------------------\n",
    "\n",
    "np.savez_compressed(\n",
    "    OUT_BUNDLE,\n",
    "    data=encoded_sparse.data,\n",
    "    indices=encoded_sparse.indices,\n",
    "    indptr=encoded_sparse.indptr,\n",
    "    shape=np.array(encoded_sparse.shape),\n",
    "    other_cols=df[other_cols].to_numpy(),\n",
    "    feat_names=np.array(feat_names, dtype=object)\n",
    ")\n",
    "\n",
    "print(\"Saved bundle:\", OUT_BUNDLE)\n",
    "print(\"\\nDONE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36033555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# ----- load sparse -----\n",
    "sparse = sp.load_npz(\"encoded_features.npz\")\n",
    "\n",
    "# ----- load feature names -----\n",
    "with open(\"encoded_feature_names.pkl\", \"rb\") as f:\n",
    "    feat_names = pickle.load(f)\n",
    "\n",
    "# ----- load other columns (CSV version) -----\n",
    "other_df = pd.read_csv(\"encoded_other_columns.csv\")\n",
    "\n",
    "# ----- load encoder -----\n",
    "with open(\"encoder.pkl\", \"rb\") as f:\n",
    "    encoder = pickle.load(f)\n",
    "\n",
    "print(\"Reload complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a93d361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_npz -> True encoded_features.npz\n",
      "feat_names -> True encoded_feature_names.pkl\n",
      "other_csv -> True encoded_other_columns.csv\n",
      "cleaned -> True cleaned_data.csv\n",
      "big_encoded -> False encoded_data.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "files = {\n",
    "    \"sparse_npz\": Path(\"encoded_features.npz\"),\n",
    "    \"feat_names\": Path(\"encoded_feature_names.pkl\"),\n",
    "    \"other_csv\": Path(\"encoded_other_columns.csv\"),\n",
    "    \"cleaned\": Path(\"cleaned_data.csv\"),\n",
    "    \"big_encoded\": Path(\"encoded_data.csv\"),\n",
    "}\n",
    "\n",
    "for k, p in files.items():\n",
    "    print(k, \"->\", p.exists(), str(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33be1ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved encoder.pkl\n",
      "\n",
      "SAVED ALL → encoded_all_bundle.npz\n",
      "This replaces all CSV/parquet files.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#   FINAL SOLUTION — SINGLE NPZ BUNDLE (NO CSV, NO PARQUET)\n",
    "# ================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "INPUT = \"cleaned_data.csv\"\n",
    "OUT_BUNDLE = \"encoded_all_bundle.npz\"\n",
    "ENCODER_PKL = \"encoder.pkl\"\n",
    "\n",
    "cat_cols = [\"city\", \"cuisine\"]\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD CLEANED DATA\n",
    "# -------------------------------\n",
    "\n",
    "df = pd.read_csv(INPUT)\n",
    "df[\"orig_index\"] = df.index\n",
    "\n",
    "other_cols = [c for c in df.columns if c not in cat_cols]\n",
    "\n",
    "# -------------------------------\n",
    "# ONE-HOT ENCODING (sparse)\n",
    "# -------------------------------\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "encoded_sparse = encoder.fit_transform(df[cat_cols])\n",
    "feat_names = encoder.get_feature_names_out(cat_cols)\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE ENCODER\n",
    "# -------------------------------\n",
    "\n",
    "with open(ENCODER_PKL, \"wb\") as f:\n",
    "    pickle.dump(encoder, f)\n",
    "print(\"Saved encoder.pkl\")\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE EVERYTHING IN ONE NPZ\n",
    "# -------------------------------\n",
    "\n",
    "np.savez_compressed(\n",
    "    OUT_BUNDLE,\n",
    "    data=encoded_sparse.data,\n",
    "    indices=encoded_sparse.indices,\n",
    "    indptr=encoded_sparse.indptr,\n",
    "    shape=np.array(encoded_sparse.shape),\n",
    "    feat_names=np.array(feat_names, dtype=object),\n",
    "    other_cols=df[other_cols].to_numpy(),\n",
    "    other_col_names=np.array(other_cols, dtype=object),\n",
    "    orig_index=df[\"orig_index\"].to_numpy()\n",
    ")\n",
    "\n",
    "print(\"\\nSAVED ALL →\", OUT_BUNDLE)\n",
    "print(\"This replaces all CSV/parquet files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "478c3948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse reconstructed: (108369, 894)\n",
      "Other columns: (108369, 10)\n",
      "First feature name: city_Abids & Koti,Hyderabad\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#   RELOAD EVERYTHING FROM SINGLE BUNDLE\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "bundle = np.load(\"encoded_all_bundle.npz\", allow_pickle=True)\n",
    "\n",
    "# reconstruct sparse matrix\n",
    "csr = sp.csr_matrix(\n",
    "    (bundle[\"data\"], bundle[\"indices\"], bundle[\"indptr\"]),\n",
    "    shape=tuple(bundle[\"shape\"])\n",
    ")\n",
    "\n",
    "# restore metadata\n",
    "feat_names = bundle[\"feat_names\"]\n",
    "other_columns = bundle[\"other_cols\"]\n",
    "other_col_names = bundle[\"other_col_names\"]\n",
    "orig_index = bundle[\"orig_index\"]\n",
    "\n",
    "print(\"Sparse reconstructed:\", csr.shape)\n",
    "print(\"Other columns:\", other_columns.shape)\n",
    "print(\"First feature name:\", feat_names[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "497a30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = sp.hstack([X_other_sparse, X_ohe], format=\"csr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e45ee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final matrix: (108369, 904)\n",
      "Total features: 904\n",
      "Saved X_final_sparse.npz\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#   FIXED ML DATASET CONSTRUCTION (NO OBJECT DTYPE ERRORS)\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load bundle\n",
    "bundle = np.load(\"encoded_all_bundle.npz\", allow_pickle=True)\n",
    "\n",
    "# sparse OHE matrix\n",
    "X_ohe = sp.csr_matrix(\n",
    "    (bundle[\"data\"], bundle[\"indices\"], bundle[\"indptr\"]),\n",
    "    shape=tuple(bundle[\"shape\"])\n",
    ")\n",
    "\n",
    "feat_names = list(bundle[\"feat_names\"])\n",
    "other_cols = bundle[\"other_cols\"]\n",
    "other_col_names = list(bundle[\"other_col_names\"])\n",
    "orig_index = bundle[\"orig_index\"]\n",
    "\n",
    "# --------------------------------------\n",
    "# FIX NON-ENCODED COLUMNS (object dtype)\n",
    "# --------------------------------------\n",
    "\n",
    "other_df = pd.DataFrame(other_cols, columns=other_col_names)\n",
    "\n",
    "# convert object columns (strings) → numeric codes\n",
    "for col in other_df.columns:\n",
    "    if other_df[col].dtype == \"object\":\n",
    "        try:\n",
    "            other_df[col] = pd.to_numeric(other_df[col])\n",
    "        except:\n",
    "            other_df[col] = other_df[col].astype(\"category\").cat.codes\n",
    "\n",
    "# now guaranteed numeric\n",
    "X_other_dense = other_df.to_numpy().astype(np.float32)\n",
    "X_other_sparse = sp.csr_matrix(X_other_dense)\n",
    "\n",
    "# --------------------------------------\n",
    "# combine other features + OHE\n",
    "# --------------------------------------\n",
    "\n",
    "X_final = sp.hstack([X_other_sparse, X_ohe], format=\"csr\")\n",
    "\n",
    "print(\"Final matrix:\", X_final.shape)\n",
    "print(\"Total features:\", X_final.shape[1])\n",
    "\n",
    "# save final\n",
    "sp.save_npz(\"X_final_sparse.npz\", X_final, compressed=True)\n",
    "print(\"Saved X_final_sparse.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd3ec357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing sparse artifacts. Proceeding to stream-combine (Path A).\n",
      "[combine] sparse shape: (108369, 894), nnz: 216738\n",
      "[combine] Wrote rows 0..1999\n",
      "[combine] Wrote rows 2000..3999\n",
      "[combine] Wrote rows 4000..5999\n",
      "[combine] Wrote rows 6000..7999\n",
      "[combine] Wrote rows 8000..9999\n",
      "[combine] Wrote rows 10000..11999\n",
      "[combine] Wrote rows 12000..13999\n",
      "[combine] Wrote rows 14000..15999\n",
      "[combine] Wrote rows 16000..17999\n",
      "[combine] Wrote rows 18000..19999\n",
      "[combine] Wrote rows 20000..21999\n",
      "[combine] Wrote rows 22000..23999\n",
      "[combine] Wrote rows 24000..25999\n",
      "[combine] Wrote rows 26000..27999\n",
      "[combine] Wrote rows 28000..29999\n",
      "[combine] Wrote rows 30000..31999\n",
      "[combine] Wrote rows 32000..33999\n",
      "[combine] Wrote rows 34000..35999\n",
      "[combine] Wrote rows 36000..37999\n",
      "[combine] Wrote rows 38000..39999\n",
      "[combine] Wrote rows 40000..41999\n",
      "[combine] Wrote rows 42000..43999\n",
      "[combine] Wrote rows 44000..45999\n",
      "[combine] Wrote rows 46000..47999\n",
      "[combine] Wrote rows 48000..49999\n",
      "[combine] Wrote rows 50000..51999\n",
      "[combine] Wrote rows 52000..53999\n",
      "[combine] Wrote rows 54000..55999\n",
      "[combine] Wrote rows 56000..57999\n",
      "[combine] Wrote rows 58000..59999\n",
      "[combine] Wrote rows 60000..61999\n",
      "[combine] Wrote rows 62000..63999\n",
      "[combine] Wrote rows 64000..65999\n",
      "[combine] Wrote rows 66000..67999\n",
      "[combine] Wrote rows 68000..69999\n",
      "[combine] Wrote rows 70000..71999\n",
      "[combine] Wrote rows 72000..73999\n",
      "[combine] Wrote rows 74000..75999\n",
      "[combine] Wrote rows 76000..77999\n",
      "[combine] Wrote rows 78000..79999\n",
      "[combine] Wrote rows 80000..81999\n",
      "[combine] Wrote rows 82000..83999\n",
      "[combine] Wrote rows 84000..85999\n",
      "[combine] Wrote rows 86000..87999\n",
      "[combine] Wrote rows 88000..89999\n",
      "[combine] Wrote rows 90000..91999\n",
      "[combine] Wrote rows 92000..93999\n",
      "[combine] Wrote rows 94000..95999\n",
      "[combine] Wrote rows 96000..97999\n",
      "[combine] Wrote rows 98000..99999\n",
      "[combine] Wrote rows 100000..101999\n",
      "[combine] Wrote rows 102000..103999\n",
      "[combine] Wrote rows 104000..105999\n",
      "[combine] Wrote rows 106000..107999\n",
      "[combine] Wrote rows 108000..108368\n",
      "[combine] Done. Wrote 108369 rows to encoded_data_dense.csv\n",
      "All done. If you produced encoded_data_dense.csv, check file size and disk space.\n"
     ]
    }
   ],
   "source": [
    "# SINGLE CELL: auto-branch -> (A) combine existing sparse artifacts OR (B) rebuild then combine\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "SPARSE_NPZ = Path(\"encoded_features.npz\")\n",
    "FEAT_NAMES_PKL = Path(\"encoded_feature_names.pkl\")\n",
    "OTHER_CSV = Path(\"encoded_other_columns.csv\")\n",
    "INPUT_CLEANED = Path(\"cleaned_data.csv\")\n",
    "OUT_DENSE = Path(\"encoded_data_dense.csv\")   # final wide CSV (be careful with disk+memory)\n",
    "ENCODER_PATH = Path(\"encoder.pkl\")\n",
    "cat_cols = [\"city\", \"cuisine\"]\n",
    "batch_size = 2000   # reduce if you get MemoryError\n",
    "# ------------------------------------------\n",
    "\n",
    "def stream_combine(sparse_path, featnames_path, other_csv, out_dense, batch_size=2000):\n",
    "    # Load metadata\n",
    "    with open(featnames_path, \"rb\") as f:\n",
    "        feat_names = pickle.load(f)\n",
    "\n",
    "    sparse_mtx = sp.load_npz(sparse_path)  # csr_matrix\n",
    "    n_rows, n_feats = sparse_mtx.shape\n",
    "    print(f\"[combine] sparse shape: {sparse_mtx.shape}, nnz: {sparse_mtx.nnz}\")\n",
    "\n",
    "    # header: other CSV columns + ohe names\n",
    "    other_hdr = pd.read_csv(other_csv, nrows=0).columns.tolist()\n",
    "    all_columns = other_hdr + list(feat_names)\n",
    "\n",
    "    # Write header\n",
    "    with open(out_dense, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        f.write(\",\".join(map(str, all_columns)) + \"\\n\")\n",
    "\n",
    "    # Check ordering: ensure 'orig_index' exists and, if necessary, reorder other_csv to numeric increasing order\n",
    "    # We will read other_csv in chunks, but to guarantee correct alignment we need to know if other_csv is in same order\n",
    "    # Quick check: read first 10 orig_index and compare with 0..9\n",
    "    other_df_head = pd.read_csv(other_csv, usecols=[\"orig_index\"], nrows=10)\n",
    "    head_is_seq = (other_df_head[\"orig_index\"].astype(int).tolist() == list(range(len(other_df_head))))\n",
    "    if not head_is_seq:\n",
    "        print(\"[combine] Note: other_csv orig_index does not look like sequential reading index. We'll reorder by orig_index before combining.\")\n",
    "        # Reorder other_csv fully in a memory-safe way into a temp file using chunks (we'll create other_csv._reordered)\n",
    "        temp_reordered = other_csv.with_name(other_csv.stem + \"_reordered.csv\")\n",
    "        # Read entire other_csv into df may be OK; if it's too big we do chunked sort via disk — but here attempt full load first with try\n",
    "        try:\n",
    "            odf = pd.read_csv(other_csv)\n",
    "            odf = odf.sort_values(\"orig_index\").reset_index(drop=True)\n",
    "            odf.to_csv(temp_reordered, index=False)\n",
    "            other_csv_used = temp_reordered\n",
    "            print(\"[combine] Reordered other_csv in-memory (wrote tmp file):\", temp_reordered)\n",
    "        except MemoryError:\n",
    "            # Fallback: perform an external sort-like merge via pandas chunks (less efficient). We'll build index->row mapping in disk chunks.\n",
    "            print(\"[combine] Reordering via chunked method due to MemoryError. This may take longer.\")\n",
    "            # Read orig_index only to determine order\n",
    "            idx_series = pd.read_csv(other_csv, usecols=[\"orig_index\"])[\"orig_index\"].astype(int)\n",
    "            # Create a DataFrame of positions to read in sorted order (this requires random-access read which CSV doesn't support)\n",
    "            # So fallback to simpler approach: read whole file but with low memory; raise helpful error for now\n",
    "            raise MemoryError(\"Chunked external reordering not implemented in this environment. Reduce batch_size or rebuild sparse and ensure other_csv order matches sparse rows.\")\n",
    "    else:\n",
    "        other_csv_used = other_csv\n",
    "\n",
    "    # Now stream in chunks and combine\n",
    "    reader = pd.read_csv(other_csv_used, chunksize=batch_size)\n",
    "    row_start = 0\n",
    "    total_written = 0\n",
    "    for chunk in reader:\n",
    "        chunk_len = len(chunk)\n",
    "        row_end = row_start + chunk_len\n",
    "        # slice sparse\n",
    "        sparse_slice = sparse_mtx[row_start:row_end]\n",
    "        dense_slice = sparse_slice.toarray()  # convert only this batch\n",
    "        ohe_df = pd.DataFrame(dense_slice, columns=feat_names, index=chunk.index)\n",
    "        out_df = pd.concat([chunk.reset_index(drop=True), ohe_df.reset_index(drop=True)], axis=1)\n",
    "        out_df.to_csv(out_dense, mode=\"a\", header=False, index=False)\n",
    "        total_written += chunk_len\n",
    "        print(f\"[combine] Wrote rows {row_start}..{row_end-1}\")\n",
    "        row_start = row_end\n",
    "\n",
    "    print(f\"[combine] Done. Wrote {total_written} rows to {out_dense}\")\n",
    "\n",
    "# --------- MAIN logic: branch ----------\n",
    "if SPARSE_NPZ.exists() and FEAT_NAMES_PKL.exists() and OTHER_CSV.exists():\n",
    "    print(\"Found existing sparse artifacts. Proceeding to stream-combine (Path A).\")\n",
    "    stream_combine(SPARSE_NPZ, FEAT_NAMES_PKL, OTHER_CSV, OUT_DENSE, batch_size=batch_size)\n",
    "else:\n",
    "    # Need to rebuild sparse artifacts from cleaned_data.csv\n",
    "    if not INPUT_CLEANED.exists():\n",
    "        raise FileNotFoundError(\"No sparse artifacts found and cleaned_data.csv not present. Cannot proceed.\")\n",
    "    print(\"Sparse artifacts not found — rebuilding from cleaned_data.csv (Path B). This will create encoded_features.npz, encoded_feature_names.pkl, encoded_other_columns.csv, encoder.pkl\")\n",
    "    df = pd.read_csv(INPUT_CLEANED)\n",
    "    df[\"orig_index\"] = df.index\n",
    "    missing = [c for c in cat_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(\"Missing categorical columns in cleaned_data.csv: \" + \", \".join(missing))\n",
    "\n",
    "    # Fit OneHotEncoder (sparse)\n",
    "    encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    encoded_sparse = encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "    # Save artifacts\n",
    "    sp.save_npz(SPARSE_NPZ, encoded_sparse, compressed=True)\n",
    "    with open(FEAT_NAMES_PKL, \"wb\") as f:\n",
    "        pickle.dump(encoder.get_feature_names_out(cat_cols), f)\n",
    "    with open(ENCODER_PATH, \"wb\") as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    other_cols = [c for c in df.columns if c not in cat_cols]\n",
    "    df[other_cols].to_csv(OTHER_CSV, index=False)\n",
    "    print(\"Rebuilt artifacts and saved. Now combining into dense CSV.\")\n",
    "    # combine now\n",
    "    stream_combine(SPARSE_NPZ, FEAT_NAMES_PKL, OTHER_CSV, OUT_DENSE, batch_size=batch_size)\n",
    "\n",
    "print(\"All done. If you produced encoded_data_dense.csv, check file size and disk space.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f737fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvimport",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
